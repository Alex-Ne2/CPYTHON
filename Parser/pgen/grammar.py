import collections


class Grammar:
    """Pgen parsing tables class.

    The instance variables are as follows:

    symbol2number -- a dict mapping symbol names to numbers.  Symbol
                     numbers are always 256 or higher, to distinguish
                     them from token numbers, which are between 0 and
                     255 (inclusive).

    number2symbol -- a dict mapping numbers to symbol names;
                     these two are each other's inverse.

    states        -- a list of DFAs, where each DFA is a list of
                     states, each state is a list of arcs, and each
                     arc is a (i, j) pair where i is a label and j is
                     a state number.  The DFA number is the index into
                     this list.  (This name is slightly confusing.)
                     Final states are represented by a special arc of
                     the form (0, j) where j is its own state number.

    dfas          -- a dict mapping symbol numbers to (DFA, first)
                     pairs, where DFA is an item from the states list
                     above, and first is a set of tokens that can
                     begin this grammar rule.

    labels        -- a list of (x, y) pairs where x is either a token
                     number or a symbol number, and y is either None
                     or a string; the strings are keywords.  The label
                     number is the index in this list; label numbers
                     are used to mark state transitions (arcs) in the
                     DFAs.

    start         -- the number of the grammar's start symbol.

    keywords      -- a dict mapping keyword strings to arc labels.

    tokens        -- a dict mapping token numbers to arc labels.

    """

    def __init__(self):
        self.symbol2number = collections.OrderedDict()
        self.number2symbol = collections.OrderedDict()
        self.states = []
        self.dfas = collections.OrderedDict()
        self.labels = [(0, "EMPTY")]
        self.keywords = collections.OrderedDict()
        self.tokens = collections.OrderedDict()
        self.symbol2label = collections.OrderedDict()
        self.start = 256

    def produce_graminit_h(self, writer):
        writer("/* Generated by Parser/pgen */\n\n")
        for number, symbol in self.number2symbol.items():
            writer("#define {} {}\n".format(symbol, number))

    def produce_graminit_c(self, writer, name):
        writer("/* Generated by Parser/pgen */\n\n")

        writer('#include "grammar.h"\n')
        writer("grammar {};\n".format(name))

        self.print_dfas(writer)
        self.print_labels(writer)

        writer("grammar {} = {{\n".format(name))
        writer("    {n_dfas},\n".format(n_dfas=len(self.dfas)))
        writer("    dfas,\n")
        writer("    {{{n_labels}, labels}},\n".format(n_labels=len(self.labels)))
        writer("    {start_number}\n".format(start_number=self.start))
        writer("};\n")

    def print_labels(self, writer):
        writer(
            "static const label labels[{n_labels}] = {{\n".format(n_labels=len(self.labels))
        )
        for label, name in self.labels:
            label_name = '"{}"'.format(name) if name is not None else 0
            writer(
                '    {{{label}, {label_name}}},\n'.format(
                    label=label, label_name=label_name
                )
            )
        writer("};\n")

    def print_dfas(self, writer):
        self.print_states(writer)
        writer("static const dfa dfas[{}] = {{\n".format(len(self.dfas)))
        for dfaindex, dfa_elem in enumerate(self.dfas.items()):
            symbol, (dfa, first_sets) = dfa_elem
            writer(
                '    {{{dfa_symbol}, "{symbol_name}", '.format(
                    dfa_symbol=symbol, symbol_name=self.number2symbol[symbol]
                )
                + "{n_states}, states_{dfa_index},".format(
                    n_states=len(dfa), dfa_index=dfaindex
                )
            )
            if first_sets is None:
                writer(" 0},\n")
                continue
            writer('\n     "')
            bitset = bytearray((len(self.labels) >> 3) + 1)
            for token in first_sets:
                bitset[token >> 3] |= 1 << (token & 7)
            for byte in bitset:
                writer("\\%03o" % (byte & 0xFF))
            writer('"},\n')
        writer("};\n")

    def print_states(self, write):
        for dfaindex, dfa in enumerate(self.states):
            self.print_arcs(write, dfaindex, dfa)
            write(
                "static state states_{dfa_index}[{n_states}] = {{\n".format(
                    dfa_index=dfaindex, n_states=len(dfa)
                )
            )
            for stateindex, state in enumerate(dfa):
                narcs = len(state)
                write(
                    "    {{{n_arcs}, arcs_{dfa_index}_{state_index}}},\n".format(
                        n_arcs=narcs, dfa_index=dfaindex, state_index=stateindex
                    )
                )
            write("};\n")

    def print_arcs(self, write, dfaindex, states):
        for stateindex, state in enumerate(states):
            narcs = len(state)
            write(
                "static const arc arcs_{dfa_index}_{state_index}[{n_arcs}] = {{\n".format(
                    dfa_index=dfaindex, state_index=stateindex, n_arcs=narcs
                )
            )
            for a, b in state:
                write(
                    "    {{{from_label}, {to_state}}},\n".format(
                        from_label=a, to_state=b
                    )
                )
            write("};\n")

    def extend_for_validation(self):
        """  Set up ValidationGrammar by extending _PyParser_Grammar with
        the "transitive closure" of single-arc productions.

        Given the grammar which has single-arc productions, such as

          or_test ::= and_test

        --and other productions using the nonterminals produced by the above, e.g.

          test := or_test 'if' or_test 'else' test

        --it adds to the grammar the "transitive" rules.
        For the rules shown above, the following new rules will be added:

          test := or_test 'if' and_test 'else' test
          test := and_test 'if' or_test 'else' test
          test := and_test 'if' and_test 'else' test
        """

        reducible_from = {symbol: set() for symbol in self.dfas}
        nt_label = {}

        # Build the reverse mapping for NT index -> label ID
        for i, (symbol, _) in enumerate(self.labels):
            if symbol > 255:
                nt_label[symbol] = i

        # Pass 1: build the sets in `reducible_from`
        for symbol, (states, _) in self.dfas.items():
            label = nt_label[symbol]
            for a_label, a_arrow in states[0]:

                # Does `a_arrow` point to a final state?
                if any(next_label == 0 for next_label, _ in states[a_arrow]):

                    # Yes! Update the sets: `symbol` is reducible from `a_label`
                    reducible_from[symbol].add(a_label)

                    lb_type, _ = self.labels[a_label]
                    if lb_type > 255:
                        # `symbol` is transitively reducible from whatever
                        # `a_label`'s symbol is reducible from.
                        reducible_from[symbol] |= reducible_from[lb_type]

                    # Anything that is reducible from `symbol`, is transitively
                    # reducible from whatever `symbol` is reducible from.
                    for other in self.dfas:
                        if label in reducible_from[other]:
                            reducible_from[other] |= reducible_from[symbol]

        # Pass 2: generate the extended DFAs
        self.states = []
        new_dfas = {}
        for symbol, (old_states, _) in self.dfas.items():
            new_states = []
            for old_state in old_states:
                new_state = []

                # Initialize the new arcs.
                for old_arc in old_state:
                    a_label, a_arrow = old_arc
                    lb_type, lb_name = self.labels[a_label]

                    # Copy the original arc; if it's a specific NAME, put it first,
                    # so that it takes precedence over a generic NAME arc.
                    if lb_name:
                        new_state.insert(0, old_arc)
                    else:
                        new_state.append(old_arc)

                    # Add new arcs for symbols that `lb_type` is reducible from.
                    if lb_type > 255:
                        new_state.extend((other_lbl, a_arrow)
                                         for other_lbl in reducible_from[lb_type])

                new_states.append(new_state)

            self.states.append(new_states)
            new_dfas[symbol] = (new_states, None)

        self.dfas = new_dfas
